\contentsline {section}{\numberline {1}2014-06-03 Channel Estimation}{4}{section.1}
\contentsline {subsection}{\numberline {1.1}Problem Statement:}{4}{subsection.1.1}
\contentsline {subsection}{\numberline {1.2}Solution:}{4}{subsection.1.2}
\contentsline {subsection}{\numberline {1.3}Multiple Copies of Y}{4}{subsection.1.3}
\contentsline {section}{\numberline {2}Another L[X|Y]}{4}{section.2}
\contentsline {section}{\numberline {3}Notes from Kalman Filter Wikipedia}{4}{section.3}
\contentsline {section}{\numberline {4}2014-06-05}{4}{section.4}
\contentsline {subsection}{\numberline {4.1}Q1}{4}{subsection.4.1}
\contentsline {subsection}{\numberline {4.2}Q2}{5}{subsection.4.2}
\contentsline {subsection}{\numberline {4.3}Q3}{5}{subsection.4.3}
\contentsline {subsection}{\numberline {4.4}Q4}{5}{subsection.4.4}
\contentsline {subsection}{\numberline {4.5}Q5}{5}{subsection.4.5}
\contentsline {section}{\numberline {5}Notes from EE126 Appendix A}{5}{section.5}
\contentsline {section}{\numberline {6}Proofs about L[X|Y]}{5}{section.6}
\contentsline {subsection}{\numberline {6.1}L[X|Y,Z] = L[X|Y] + L[X|Z]}{5}{subsection.6.1}
\contentsline {subsection}{\numberline {6.2}L[X|Y,Z] = L[X|Y] + L[X|Z-L[Z|Y]]}{5}{subsection.6.2}
\contentsline {section}{\numberline {7}2014-06-09\nobreakspace {}2014-06-15 Kalman Filter}{5}{section.7}
\contentsline {subsection}{\numberline {7.1}Problem Setup:}{5}{subsection.7.1}
\contentsline {subsection}{\numberline {7.2}Goal:}{5}{subsection.7.2}
\contentsline {subsection}{\numberline {7.3}Equations:}{5}{subsection.7.3}
\contentsline {subsection}{\numberline {7.4}$\mathbb {E}[X[n+1]|Y^{n-1}]$}{6}{subsection.7.4}
\contentsline {subsection}{\numberline {7.5}$\mathbb {E}[Y[n]|Y^{n-1}]$}{6}{subsection.7.5}
\contentsline {subsection}{\numberline {7.6}$\mathbb {E}[X[n+1]|Y[n]-C\mathaccentV {hat}05E{X}[n]]$}{6}{subsection.7.6}
\contentsline {section}{\numberline {8}2014-06-16 Underlying X1 and X2}{7}{section.8}
\contentsline {subsection}{\numberline {8.1}Problem Setup}{7}{subsection.8.1}
\contentsline {subsection}{\numberline {8.2}Solution}{7}{subsection.8.2}
\contentsline {section}{\numberline {9}2014-06-18 Conditions of Observability}{7}{section.9}
\contentsline {section}{\numberline {10}2014-06-19 Various Proofs}{7}{section.10}
\contentsline {subsection}{\numberline {10.1}Best Control}{7}{subsection.10.1}
\contentsline {subsection}{\numberline {10.2}Error of Control Problem}{7}{subsection.10.2}
\contentsline {subsection}{\numberline {10.3}Without System Error, Estimation Error = 0}{8}{subsection.10.3}
\contentsline {section}{\numberline {11}2014-06-29 Kalman Filter with Multiplicative Noise}{8}{section.11}
\contentsline {subsection}{\numberline {11.1}Problem Setup:}{8}{subsection.11.1}
\contentsline {subsection}{\numberline {11.2}Goal:}{8}{subsection.11.2}
\contentsline {subsection}{\numberline {11.3}Equations:}{8}{subsection.11.3}
\contentsline {subsection}{\numberline {11.4}$\mathbb {E}[X[n]|Y^{n-1}]$}{8}{subsection.11.4}
\contentsline {subsection}{\numberline {11.5}$\mathbb {E}[Y[n]|Y^{n-1}]$}{8}{subsection.11.5}
\contentsline {subsection}{\numberline {11.6}$\mathbb {E}[X[n]|Y[n]-CM(n)A\mathaccentV {hat}05E{X}[n-1]]$}{8}{subsection.11.6}
\contentsline {section}{\numberline {12}Notes from A Mathematical Theory of Communication}{9}{section.12}
\contentsline {subsubsection}{\numberline {12.0.1}Introduction}{9}{subsubsection.12.0.1}
\contentsline {subsection}{\numberline {12.1}Discrete Noiseless Systems}{10}{subsection.12.1}
\contentsline {subsubsection}{\numberline {12.1.1}The Discrete Noiseless Channel}{10}{subsubsection.12.1.1}
\contentsline {subsubsection}{\numberline {12.1.2}The Discrete Source of Information}{10}{subsubsection.12.1.2}
\contentsline {subsubsection}{\numberline {12.1.3}The Series of Approximations to English}{10}{subsubsection.12.1.3}
\contentsline {subsubsection}{\numberline {12.1.4}Graphical Representation of a Markoff (Markov?) Process}{10}{subsubsection.12.1.4}
\contentsline {subsubsection}{\numberline {12.1.5}Ergodic and Mixed Sources}{10}{subsubsection.12.1.5}
\contentsline {subsubsection}{\numberline {12.1.6}Choice, Uncertainty and Entropy}{11}{subsubsection.12.1.6}
\contentsline {subsubsection}{\numberline {12.1.7}The Entropy of an Information Source}{11}{subsubsection.12.1.7}
\contentsline {subsubsection}{\numberline {12.1.8}Representations of the Encoding and Decoding Operations}{12}{subsubsection.12.1.8}
\contentsline {subsubsection}{\numberline {12.1.9}The Fundamental Theorem for a Noiseless Channel}{12}{subsubsection.12.1.9}
\contentsline {subsubsection}{\numberline {12.1.10}Discussion and Examples}{12}{subsubsection.12.1.10}
\contentsline {subsection}{\numberline {12.2}Discrete Noisy Systems}{12}{subsection.12.2}
\contentsline {subsubsection}{\numberline {12.2.1}Representation of a Noisy Discrete Channel}{12}{subsubsection.12.2.1}
\contentsline {subsubsection}{\numberline {12.2.2}Equivocation and Channel Capacity}{12}{subsubsection.12.2.2}
\contentsline {subsubsection}{\numberline {12.2.3}The Fundamental Theorem for a Discrete Channel with Noise}{12}{subsubsection.12.2.3}
\contentsline {subsubsection}{\numberline {12.2.4}Discussion}{13}{subsubsection.12.2.4}
\contentsline {subsubsection}{\numberline {12.2.5}Example of a Discrete Channel and its Capacity}{13}{subsubsection.12.2.5}
\contentsline {subsubsection}{\numberline {12.2.6}The Channel Capacity in Certain Special Cases}{13}{subsubsection.12.2.6}
\contentsline {subsubsection}{\numberline {12.2.7}An Example of Efficient Coding}{13}{subsubsection.12.2.7}
\contentsline {section}{\numberline {13}Yury Polyanskiy: Channel Coding Rate in the Finite Blocklength Regime}{13}{section.13}
\contentsline {section}{\numberline {14}Cover and Thomas: Information Theory}{13}{section.14}
\contentsline {subsection}{\numberline {14.1}Chapter 1: Introduction and Preview}{13}{subsection.14.1}
\contentsline {subsection}{\numberline {14.2}Chapter 2: Entropy and Mutual Information}{14}{subsection.14.2}
\contentsline {subsubsection}{\numberline {14.2.1}Entropy}{14}{subsubsection.14.2.1}
\contentsline {subsubsection}{\numberline {14.2.2}Joint and Conditional Entropy}{14}{subsubsection.14.2.2}
\contentsline {subsubsection}{\numberline {14.2.3}Relative Entropy and Mutual Information}{14}{subsubsection.14.2.3}
\contentsline {subsubsection}{\numberline {14.2.4}Relationship between Entropy and Mutual Information}{14}{subsubsection.14.2.4}
\contentsline {subsubsection}{\numberline {14.2.5}Chain Rules}{14}{subsubsection.14.2.5}
\contentsline {section}{\numberline {15}2014-07-14\nobreakspace {}2014-07-21 Kalman Filter Learning Crand}{14}{section.15}
\contentsline {section}{\numberline {16}Results of Simulations}{16}{section.16}
\contentsline {subsection}{\numberline {16.1}Kalman Filter Additive Noise}{16}{subsection.16.1}
\contentsline {subsection}{\numberline {16.2}Kalman Filter Ignoring Multiplicative Noise}{17}{subsection.16.2}
\contentsline {subsection}{\numberline {16.3}Kalman Filter for Multiplicative Noise}{20}{subsection.16.3}
\contentsline {subsection}{\numberline {16.4}Quantization Noise: Schenato}{22}{subsection.16.4}
\contentsline {subsection}{\numberline {16.5}NonCoherence Estimation: Gireeja}{25}{subsection.16.5}
\contentsline {subsection}{\numberline {16.6}NonCoherence Control: Gireeja}{27}{subsection.16.6}
\contentsline {subsection}{\numberline {16.7}Varying A, NonCoherence Control, Gireeja}{28}{subsection.16.7}
\contentsline {subsection}{\numberline {16.8}Rajasekaran vs Schenato: Prediction vs Estimation}{29}{subsection.16.8}
